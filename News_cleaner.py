import pandas as pd
import re
import json
import os
import jieba
import jieba.analyse

INPUT_FILE = "ettoday_raw_data.csv"
OUTPUT_JSON = "cleaned_news.json"

#å®šç¾©ã€Œåƒåœ¾è©ã€é»‘åå–® ---
# é€™äº›è©é›–ç„¶å‡ºç¾é »ç‡é«˜ï¼Œä½†å°åˆ†ææ²’å¹«åŠ©ï¼Œè¦æŠŠå®ƒå€‘éæ¿¾æ‰
# 1. é—œéµè©é»‘åå–® (éæ¿¾æ‰æ²’æ„ç¾©çš„è©)
STOP_WORDS = {
    "è¨˜è€…", "å ±å°", "ç¿»æ”", "åœ–æ–‡", "æ¡è¨ª", "ç¶œåˆ", "ä¸­å¿ƒ", "ç·¨è¼¯", 
    "ä¾†æº", "ç•«é¢", "æ›å…‰", "æŒ‡å‡º", "è¡¨ç¤º", "èªç‚º", "ä»Šæ—¥", "æ˜¨æ—¥",
    "å°ç£", "å°åŒ—", "ETtoday", "æ–°èé›²", "å¯ä»¥", "æˆ‘å€‘", "æ‡‰è©²","ä¸€èµ·",
    "é€™äº›", "é‚£äº›", "éå¸¸", "éå¸¸", "å¾ˆå¤š", "çœ‹åˆ°", "çŸ¥é“", "æ™‚é–“",
    "åœ°æ–¹", "äº‹æƒ…", "å•é¡Œ", "åŸå› ", "æ–¹å¼", "æ–¹æ³•", "æƒ…æ³", "æƒ…å½¢",
    "æ´»å‹•", "å…¬å¸", "æ”¿åºœ", "æ°‘çœ¾", "å­¸ç”Ÿ", "å®¶äºº", "æœ‹å‹", "ç”Ÿæ´»", "å·¥ä½œ",
    "ç¤¾æœƒ", "æ–‡åŒ–", "ç¶“æ¿Ÿ", "æ”¿æ²»", "åœ‹éš›", "åœ°å€", "åœ°é»", "åœ°çƒ","ä¸–ç•Œ",
    "æ–°è", "å ±å°", "æ¶ˆæ¯", "è³‡è¨Š", "è³‡æ–™", "å…§å®¹", "æ¨™é¡Œ", "æ–‡ç« ","å¿«è¨Š",
    "å½±ç‰‡", "åœ–ç‰‡", "ç…§ç‰‡", "ç¶²å‹", "ç•™è¨€", "åˆ†äº«", "é—œæ³¨", "ç†±é–€",
}

# 2. è¨˜è€…é»‘åå–® (æ–°å¢ï¼šæä¾›ã€ç¿»æ”ã€å–è‡ª...ç­‰åœ–ç‰‡ä¾†æºè©)
REPORTER_BLACKLIST = {
    # åª’é«”/å–®ä½å
    "7Car", "å°ä¸ƒè»Šè§€é»", "ä¸­å¤®ç¤¾", "å¤–é›»", "å ±å°", "æ•´ç†", 
    "æ–°èé›²", "ä¸­å¿ƒ", "ç·¨è¼¯", "ç¶²æœ", "ç¤¾ç¾¤", "å°çµ„",
    
    # åœ–ç‰‡/ä¾†æºç”¨èª
    "æä¾›", "ç¿»æ”", "ç¤ºæ„åœ–", "æˆªå–", "å–è‡ª", "ç•«é¢", "ç²‰å°ˆ", 
    "è‡‰æ›¸", "IG", "Youtube", "æ°‘çœ¾", "ç¶²å‹", "Facebook",

    # æ–°å¢ï¼šè·ç¨±èˆ‡å¤šé¤˜è³‡è¨Š
    "æ”å½±", "å‰ªè¼¯", "è£½ä½œ", "æ’°æ–‡", "å°ˆæ¬„", "è¨˜è€…æ”å½±", "è¨˜è€…å‰ªè¼¯",
    "è¨˜è€…æ’°æ–‡", "è¨˜è€…å°ˆæ¬„","è¨˜è€…å ±å°","è¨˜è€…æ•´ç†","è¨˜è€…ä¸­å¿ƒ","è¨˜è€…ç¶²æœ",
    "è¨˜è€…ç¤¾ç¾¤","è¨˜è€…å°çµ„"
}


def extract_keywords_from_text(text):
    if not text or pd.isna(text):
        return []
    
    raw_keywords = jieba.analyse.extract_tags(text, topK=50)
    
    filtered_keywords = []
    for w in raw_keywords:
        # --- éæ¿¾é‚è¼¯ ---
        # 1. å¿…é ˆä¸åœ¨é»‘åå–®
        # 2. é•·åº¦ > 1 (éæ¿¾å–®å­—)
        # 3. ä¸èƒ½æ˜¯ç´”æ•¸å­— (æ–°å¢åŠŸèƒ½ï¼Œéæ¿¾ "20", "10")
        if w not in STOP_WORDS and len(w) > 1 and not w.isdigit():
            filtered_keywords.append(w)
    
    return filtered_keywords[:5] # æœ€å¾Œåªå–å‰ 5 å€‹

def extract_reporter(content):
    if pd.isna(content): return "Unknown"
    patterns = [
        r"è¨˜è€…(.*?)[ï¼|/]", 
        r"æ–‡[ï¼|/](.*?)[\s|ï¼Œ|ã€‚]",
        r"åœ–ã€æ–‡[ï¼|/](.*?)\)",
    ]
    for pattern in patterns:
        match = re.search(pattern, content)
        if match:
            name = match.group(1).strip()
            # æ’é™¤æ˜é¡¯éŒ¯èª¤çš„çµæœ
            
            # 1. ç¬¦è™Ÿæª¢æŸ¥ï¼šåå­—è£¡ä¸è©²æœ‰æ¨™é»ç¬¦è™Ÿ
            if any(char in name for char in ["(", ")", "ã€‚", "ã€", "ï¼Œ", "ï¼", "?", "ã€", "ã€‘", "ï¼", "/", "ï¼›", ";", ":", "ï¼š"]):
                continue
            
            # 2. é•·åº¦æª¢æŸ¥ï¼šå¤ªçŸ­æˆ–å¤ªé•·éƒ½ä¸åƒäººå
            # ä¸­æ–‡åé€šå¸¸ 2-4 å­—ï¼Œè‹±æ–‡å(å¦‚ Kolas) å¯èƒ½é•·ä¸€é»ï¼Œä½†ä¸æœƒå¤ªé•·
            if len(name) < 2 or len(name) > 10:
                continue

            # 3. é»‘åå–®æª¢æŸ¥ ï¼šéæ¿¾æ‰å¸¸è¦‹çš„éäººåè©
            if any(blk in name for blk in REPORTER_BLACKLIST):
                continue

            # 4. é›™é‡ç¢ºèªï¼šæœ‰äº›å¥‡æ€ªçš„ "åœ–ï¼" æœƒæŠ“åˆ°éäººå
            # å¦‚æœåå­—è£¡é¢æœ‰ "åœ–"ï¼Œé€šå¸¸æ˜¯æŠ“éŒ¯äº† (ä¾‹å¦‚ "åœ–ï¼è¨˜è€…...")
            if "åœ–" in name:
                continue

            return name
    return "Unknown"

def clean_data():
    print(f"ğŸ§¹ é–‹å§‹è®€å– raw data: {INPUT_FILE}")
    if not os.path.exists(INPUT_FILE):
        print("âŒ æ‰¾ä¸åˆ° raw dataï¼Œè«‹å…ˆåŸ·è¡Œçˆ¬èŸ²ï¼")
        return

    df = pd.read_csv(INPUT_FILE)
    df.drop_duplicates(subset=['link'], inplace=True)
    df.dropna(subset=['title', 'content'], inplace=True)
    
    print("ğŸ” æ­£åœ¨æå–è³‡æ–™ (è¨˜è€… & é—œéµè©)...")
    
    df['reporter'] = df['content'].apply(extract_reporter)

    print("ğŸ” æ­£åœ¨å¾ã€Œæ¨™é¡Œã€æå–é—œéµè©...")
    
    df['keywords'] = df['title'].apply(extract_keywords_from_text)
    
    final_df = df[['title', 'content', 'date_str', 'category', 'reporter', 'link', 'keywords']]
    
    json_data = final_df.to_dict(orient='records')
    
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=4)
        
    print(f"âœ¨ æ¸…æ´—å®Œæˆï¼æª”æ¡ˆå·²å­˜ç‚º: {OUTPUT_JSON}")
    # é è¦½ä¸€ä¸‹ï¼Œç¢ºèªã€Œè¨˜è€…ã€é€™ç¨®è©æœ‰æ²’æœ‰æ¶ˆå¤±
    print("ğŸ‘€ é—œéµè©ç¯„ä¾‹:", final_df.iloc[0]['keywords'])

if __name__ == "__main__":
    clean_data()