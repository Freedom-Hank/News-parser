from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime, timedelta
import os
import urllib3

# 1. é—œé–‰ SSL å®‰å…¨æ†‘è­‰è­¦å‘Š (é—œéµä¿®æ­£)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- è¨­å®šå€ ---
START_DATE = "2025-12-15" 
DAYS_TO_CRAWL = 1  # å…ˆè¨­ 2 å¤©è©¦è·‘ï¼Œç¢ºèª OK å¾Œå†æ”¹æˆ 20
OUTPUT_FILE = "ettoday_raw_data.csv"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.ettoday.net/",
}

def get_news_links_by_date(date_str):
    url = f"https://www.ettoday.net/news/news-list-{date_str}-0.htm"
    print(f"\nğŸ“¡ [Selenium] æ­£åœ¨é–‹å•Ÿç€è¦½å™¨æŠ“å–åˆ—è¡¨: {url}")
    
    # æ ¼å¼åŒ–æ—¥æœŸä»¥ä¾¿æ¯”å° (ETtoday ç¶²é é¡¯ç¤ºçš„æ˜¯ 2024/03/20ï¼Œè€Œæˆ‘å€‘è¼¸å…¥çš„æ˜¯ 2024-03-20)
    target_date_slash = date_str.replace("-", "/") 
    
    chrome_options = Options()
    chrome_options.add_argument("--disable-gpu")
    # chrome_options.add_argument("--headless") # å»ºè­°é™¤éŒ¯æ™‚å…ˆé—œæ‰ headless

    try:
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()), 
            options=chrome_options
        )
    except Exception as e:
        print(f"âŒ ç€è¦½å™¨å•Ÿå‹•å¤±æ•—: {e}")
        return []
    
    try:
        driver.get(url)
        time.sleep(2)
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        retry_count = 0
        MAX_RETRIES = 3
        
        while True:
            # 1. åŸ·è¡Œæ²å‹•
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # --- æ–°å¢é‚è¼¯ï¼šæª¢æŸ¥æœ€å¾Œä¸€å‰‡æ–°èçš„æ—¥æœŸ ---
            try:
                # æŠ“å–ç•«é¢ä¸Šæ‰€æœ‰çš„æ—¥æœŸæ¨™ç±¤ (.date)
                date_elements = driver.find_elements(By.CSS_SELECTOR, ".part_list_2 .date")
                
                if date_elements:
                    # æŠ“æœ€å¾Œä¸€å€‹å…ƒç´ çš„æ–‡å­— (ä¾‹å¦‚: "2024/03/20 12:30")
                    last_date_text = date_elements[-1].text.strip()
                    
                    # å–å‡ºæ—¥æœŸéƒ¨åˆ† (å‰é¢ 10 å€‹å­—: "2024/03/20")
                    current_date_on_page = last_date_text[:10]
                    
                    # æ¯”å°ï¼šå¦‚æœé é¢ä¸Šçš„æœ€å¾Œæ—¥æœŸ ä¸ç­‰æ–¼ ç›®æ¨™æ—¥æœŸ (ä»£è¡¨å·²ç¶“æ»‘éé ­ï¼Œæ»‘åˆ°å‰ä¸€å¤©äº†)
                    if current_date_on_page != target_date_slash:
                        print(f"   ğŸ›‘ åµæ¸¬åˆ°å‰ä¸€æ—¥æ–°è ({last_date_text})ï¼Œåœæ­¢æ²å‹•ã€‚")
                        break
            except Exception as e:
                # å¶çˆ¾æŠ“ä¸åˆ°å…ƒç´ ä¸å½±éŸ¿å¤§å±€ï¼Œç¹¼çºŒæ»‘
                pass
            # -------------------------------------

            # 2. æª¢æŸ¥é«˜åº¦æ˜¯å¦è®ŠåŒ– (åŸæœ¬çš„é‡è©¦é‚è¼¯)
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                retry_count += 1
                print(f"   âš ï¸ é«˜åº¦æœªè®ŠåŒ–ï¼Œç¬¬ {retry_count}/{MAX_RETRIES} æ¬¡é‡è©¦...")
                if retry_count >= MAX_RETRIES:
                    print("   ğŸ›‘ å·²é”é‡è©¦ä¸Šé™ï¼Œåœæ­¢æ²å‹•ã€‚")
                    break
                else:
                    time.sleep(2)
                    continue
            else:
                retry_count = 0
                last_height = new_height
                
    except Exception as e:
        print(f"âš ï¸ Selenium åŸ·è¡ŒæœŸé–“ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []
    finally:
        if 'driver' in locals():
            driver.quit()
        
    # --- è§£æ HTML ---
    html_source = driver.page_source if 'driver' in locals() else ""
    soup = BeautifulSoup(html_source, "html.parser")
    
    news_list = []
    # é€™è£¡ä¹Ÿè¦åšéæ¿¾ï¼Œç¢ºä¿æœ€å¾Œå­˜é€²å»çš„çœŸçš„åªæœ‰ç•¶å¤©çš„
    for item in soup.select(".part_list_2 > h3"):
        try:
            date_time = item.select_one(".date").text.strip() # "2024/03/20 12:30"
            
            # äºŒæ¬¡ç¢ºèªï¼šåªæ”¶éŒ„ç•¶å¤©æ—¥æœŸ
            if target_date_slash not in date_time:
                continue

            category = item.select_one("em").text.strip()
            a_tag = item.select_one("a")
            title = a_tag.text.strip()
            href = a_tag["href"]
            
            if href.startswith("http"):
                link = href
            else:
                link = "https://www.ettoday.net" + href
            
            news_list.append({
                "date_str": date_time,
                "category": category,
                "title": title,
                "link": link
            })
        except AttributeError:
            continue
    
    print(f"âœ… {date_str} æœ€çµ‚æ•´ç†å‡º {len(news_list)} å‰‡æ–°è")
    return news_list

def get_news_content(url):
    """æŠ“å–å…§æ–‡ (é–‹å•Ÿé™¤éŒ¯æ¨¡å¼)"""
    try:
        # print(f"DEBUG: å˜—è©¦æŠ“å– {url}") # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼ŒæŠŠé€™è¡Œè¨»è§£æ‰“é–‹çœ‹ç¶²å€å°ä¸å°
        
        resp = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        
        if resp.status_code != 200:
            print(f"âš ï¸ è«‹æ±‚å¤±æ•— ({resp.status_code}): {url}")
            return None
        
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        
        story_div = soup.select_one("div.story")
        if not story_div:
            story_div = soup.select_one("div.subject_article")
            
        if story_div:
            paragraphs = [p.text.strip() for p in story_div.select("p") if p.text.strip()]
            content = "\n".join(paragraphs)
            return content
        else:
            # å°å‡ºå¤±æ•—åŸå› 
            print(f"âš ï¸ æ‰¾ä¸åˆ°å…§æ–‡å€å¡Š (div.story): {url}")
            return None 

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ {url}: {e}")
        return None

# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    start = datetime.strptime(START_DATE, "%Y-%m-%d")
    date_list = [(start - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(DAYS_TO_CRAWL)]

    total_count = 0

    for date in date_list:
        print(f"ğŸš€ æ—¥æœŸ: {date}")
        
        news_items = get_news_links_by_date(date)
        
        if not news_items:
            continue

        data_for_csv = []
        
        # ä½¿ç”¨ enumerate æ–¹ä¾¿çœ‹é€²åº¦
        for i, news in enumerate(news_items):
            content = get_news_content(news["link"])
            
            if content:
                news["content"] = content
                data_for_csv.append(news)
                
                # æ¯ 50 ç¯‡å°ä¸€æ¬¡é€²åº¦ï¼Œè®“ä½ çŸ¥é“å®ƒé‚„æ´»è‘—
                if i % 50 == 0:
                    print(f"  - ({i}/{len(news_items)}) æˆåŠŸæŠ“å–: {news['title'][:15]}...")
            else:
                # æŠ“ä¸åˆ°å…§æ–‡å°±è·³éï¼Œä¸å­˜
                pass
            
            # éš¨æ©Ÿä¼‘æ¯ 0.5 ~ 1 ç§’
            time.sleep(random.uniform(0.5, 1.0))

        # è©²æ—¥æœŸè·‘å®Œï¼Œå­˜å…¥ CSV
        if data_for_csv:
            df = pd.DataFrame(data_for_csv)
            file_exists = os.path.isfile(OUTPUT_FILE)
            # è¿½åŠ æ¨¡å¼ 'a'
            df.to_csv(OUTPUT_FILE, mode='a', header=not file_exists, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ {date} å­˜æª”å®Œæˆï¼æ–°å¢ {len(df)} ç­†è³‡æ–™")
            total_count += len(df)
        
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç¸½å…±ç´¯ç© {total_count} ç­†è³‡æ–™åœ¨ {OUTPUT_FILE}")