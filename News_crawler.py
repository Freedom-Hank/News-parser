from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime, timedelta, timezone
import os
import urllib3

# 1. é—œé–‰ SSL å®‰å…¨æ†‘è­‰è­¦å‘Š (é—œéµä¿®æ­£)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- è¨­å®šå€ ---
# 1. è¨­å®šå°ç£æ™‚å€ (UTC+8)
# é€™æ¨£ç„¡è«–ä½ çš„ç¨‹å¼åœ¨ç¾åœ‹ä¸»æ©Ÿé‚„æ˜¯åœ¨å“ªè£¡è·‘ï¼Œæ°¸é éƒ½æ˜¯æŠ“å°ç£çš„ã€Œä»Šå¤©ã€
tw_timezone = timezone(timedelta(hours=8))
today_in_taiwan = datetime.now(tw_timezone)

# 2. è½‰æˆæ–‡å­—æ ¼å¼ "2025-12-16"
START_DATE = today_in_taiwan.strftime("%Y-%m-%d")

# 3. æ¯æ¬¡åªæŠ“ç•¶å¤© (å› ç‚ºä½ æ¯ 6 å°æ™‚å°±æœƒè·‘ä¸€æ¬¡ä¾†æ›´æ–°)
DAYS_TO_CRAWL = 1 

OUTPUT_FILE = "ettoday_raw_data.csv"

print(f"ğŸ¤– è‡ªå‹•åŒ–å•Ÿå‹•ï¼šç›®æ¨™æ—¥æœŸç‚º {START_DATE} (å°ç£æ™‚é–“)")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.ettoday.net/",
}

def get_news_links_by_date(date_str):
    url = f"https://www.ettoday.net/news/news-list-{date_str}-0.htm"
    print(f"\nğŸ“¡ [Selenium] æ­£åœ¨é–‹å•Ÿç€è¦½å™¨æŠ“å–åˆ—è¡¨: {url}")
    
    target_date_slash = date_str.replace("-", "/") 
    
    chrome_options = Options()
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--headless") 

    html_source = "" # 1. å…ˆå®£å‘Šé€™å€‹è®Šæ•¸

    try:
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()), 
            options=chrome_options
        )
        
        driver.get(url)
        time.sleep(2)
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        retry_count = 0
        MAX_RETRIES = 3
        
        while True:
            # æ²å‹•é‚è¼¯
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(0.5)
            driver.execute_script("window.scrollBy(0, -300);")
            time.sleep(2)
            
            # æ—¥æœŸæª¢æŸ¥
            try:
                date_elements = driver.find_elements(By.CSS_SELECTOR, ".part_list_2 .date")
                if date_elements:
                    last_date_text = date_elements[-1].text.strip()
                    current_date_on_page = last_date_text[:10]
                    if current_date_on_page != target_date_slash:
                        print(f"   ğŸ›‘ åµæ¸¬åˆ°å‰ä¸€æ—¥æ–°è ({last_date_text})ï¼Œåœæ­¢æ²å‹•ã€‚")
                        break
            except Exception:
                pass

            # é«˜åº¦æª¢æŸ¥
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                retry_count += 1
                print(f"   âš ï¸ é«˜åº¦æœªè®ŠåŒ–ï¼Œç¬¬ {retry_count}/{MAX_RETRIES} æ¬¡é‡è©¦...")
                if retry_count >= MAX_RETRIES:
                    print("   ğŸ›‘ å·²é”é‡è©¦ä¸Šé™ï¼Œåœæ­¢æ²å‹•ã€‚")
                    break
                else:
                    time.sleep(2)
                    continue
            else:
                retry_count = 0
                last_height = new_height
        
        # 2.åœ¨ç€è¦½å™¨é‚„æ´»è‘—çš„æ™‚å€™ï¼ŒæŠŠåŸå§‹ç¢¼å­˜é€²è®Šæ•¸
        print("   ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ç¶²é åŸå§‹ç¢¼...")
        html_source = driver.page_source 

    except Exception as e:
        print(f"âš ï¸ Selenium åŸ·è¡ŒæœŸé–“ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []
    
    finally:
        if 'driver' in locals():
            driver.quit()
        
    # --- è§£æ HTML ---
    
    # 3. çµ•å°ä¸è¦å†å‘¼å« driver.page_sourceï¼Œç›´æ¥ç”¨ä¸Šé¢å­˜å¥½çš„ html_source
    if not html_source:
        print("âŒ æœªå–å¾—ç¶²é åŸå§‹ç¢¼ï¼Œè·³éè§£æã€‚")
        return []

    soup = BeautifulSoup(html_source, "html.parser")
    
    news_list = []
    for item in soup.select(".part_list_2 > h3"):
        try:
            date_time = item.select_one(".date").text.strip()
            
            if target_date_slash not in date_time:
                continue

            category = item.select_one("em").text.strip()
            a_tag = item.select_one("a")
            title = a_tag.text.strip()
            href = a_tag["href"]
            
            if href.startswith("http"):
                link = href
            else:
                link = "https://www.ettoday.net" + href
            
            news_list.append({
                "date_str": date_time,
                "category": category,
                "title": title,
                "link": link
            })
        except AttributeError:
            continue
    
    print(f"âœ… {date_str} æœ€çµ‚æ•´ç†å‡º {len(news_list)} å‰‡æ–°è")
    return news_list

def get_news_content(url):
    """æŠ“å–å…§æ–‡ (é–‹å•Ÿé™¤éŒ¯æ¨¡å¼)"""
    try:
        # print(f"DEBUG: å˜—è©¦æŠ“å– {url}") # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼ŒæŠŠé€™è¡Œè¨»è§£æ‰“é–‹çœ‹ç¶²å€å°ä¸å°
        
        resp = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        
        if resp.status_code != 200:
            print(f"âš ï¸ è«‹æ±‚å¤±æ•— ({resp.status_code}): {url}")
            return None
        
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        
        story_div = soup.select_one("div.story")
        if not story_div:
            story_div = soup.select_one("div.subject_article")
            
        if story_div:
            paragraphs = [p.text.strip() for p in story_div.select("p") if p.text.strip()]
            content = "\n".join(paragraphs)
            return content
        else:
            # å°å‡ºå¤±æ•—åŸå› 
            print(f"âš ï¸ æ‰¾ä¸åˆ°å…§æ–‡å€å¡Š (div.story): {url}")
            return None 

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ {url}: {e}")
        return None

# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    start = datetime.strptime(START_DATE, "%Y-%m-%d")
    date_list = [(start - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(DAYS_TO_CRAWL)]

    total_count = 0

    for date in date_list:
        print(f"ğŸš€ æ—¥æœŸ: {date}")
        
        news_items = get_news_links_by_date(date)
        
        if not news_items:
            continue

        data_for_csv = []
        
        # ä½¿ç”¨ enumerate æ–¹ä¾¿çœ‹é€²åº¦
        for i, news in enumerate(news_items):
            content = get_news_content(news["link"])
            
            if content:
                news["content"] = content
                data_for_csv.append(news)
                
                # æ¯ 50 ç¯‡å°ä¸€æ¬¡é€²åº¦ï¼Œè®“ä½ çŸ¥é“å®ƒé‚„æ´»è‘—
                if i % 50 == 0:
                    print(f"  - ({i}/{len(news_items)}) æˆåŠŸæŠ“å–: {news['title'][:15]}...")
            else:
                # æŠ“ä¸åˆ°å…§æ–‡å°±è·³éï¼Œä¸å­˜
                pass
            
            # éš¨æ©Ÿä¼‘æ¯ 0.5 ~ 1 ç§’
            time.sleep(random.uniform(0.5, 1.0))

        # è©²æ—¥æœŸè·‘å®Œï¼Œå­˜å…¥ CSV
        if data_for_csv:
            df = pd.DataFrame(data_for_csv)
            file_exists = os.path.isfile(OUTPUT_FILE)
            # è¿½åŠ æ¨¡å¼ 'a'
            df.to_csv(OUTPUT_FILE, mode='a', header=not file_exists, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ {date} å­˜æª”å®Œæˆï¼æ–°å¢ {len(df)} ç­†è³‡æ–™")
            total_count += len(df)
        
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç¸½å…±ç´¯ç© {total_count} ç­†è³‡æ–™åœ¨ {OUTPUT_FILE}")